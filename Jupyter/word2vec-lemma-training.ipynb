{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texto a vectores (Lemma)\n",
    "\n",
    "**Descripción :**\n",
    "\n",
    "Una vez que ya tenemos nuestros datos limpios, lo que procede es convertirlos a vectores para poder utilizarlos en nuestro modelo de aprendizaje. Utilizaremos word2vec para convertir nuestros textos a vectores y también haremos la matriz TF-IDF para poder comparar los resultados.\n",
    "\n",
    "Word2Vec form Google : https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "**Resources:**\n",
    "* https://medium.com/swlh/word-embedding-word2vec-with-genism-nltk-and-t-sne-visualization-43eae8ab3e2e\n",
    "* https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "**Team:**  \n",
    "* Chaparro Sicardo Tanibeth  \n",
    "* Malváez Flores Axel Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías que necesitaremos para el procedimiento\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import word2vec, Word2Vec, KeyedVectors\n",
    "from utils import *\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data for Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Realizamos el mismo proceso para nuestro texto lematizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos\n",
    "data = pd.read_csv('../Data/lemma_df.csv')\n",
    "\n",
    "# Regex para eliminar los corchetes del string\n",
    "regex = re.compile(r'[\\[\\]]')\n",
    "\n",
    "# Aplicamos la regex a la columna 'stemming_description' y hacemos un split sobre la coma\n",
    "data['lemma_description'] = data['lemma_description'].apply(lambda x: regex.sub('', x).split(', '))\n",
    "\n",
    "# Creamos el corpus\n",
    "corpus = data['lemma_description']\n",
    "corpus = corpus.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Skip-gram\n",
    "model_sg_lemma = Word2Vec(corpus, window=20, min_count=2, workers=4, vector_size=300, sg=1)\n",
    "model_sg_lemma.save('../Data/Word2Vec-Own/word2vecSGLemma.model')\n",
    "\n",
    "word_vectors_sg_lemma = model_sg_lemma.wv\n",
    "word_vectors_sg_lemma.save('../Data/Word2Vec-Own/word2vecSGLemma.wordvectors')\n",
    "\n",
    "del model_sg_lemma\n",
    "\n",
    "########### CBOW\n",
    "model_cbow_lemma = Word2Vec(corpus, window=20, min_count=2, workers=4, vector_size=300, sg=0)\n",
    "model_cbow_lemma.save('../Data/Word2Vec-Own/word2vecCBOWLemma.model')\n",
    "\n",
    "word_vectors_cbow_lemma = model_cbow_lemma.wv\n",
    "word_vectors_cbow_lemma.save('../Data/Word2Vec-Own/word2vecCBLemma.wordvectors')\n",
    "\n",
    "del model_cbow_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de nuestra matriz de documentos (ya vectorizados): (50424, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load back with memory-mapping = read-only, shared across processes.\n",
    "wv = KeyedVectors.load('../Data/Word2Vec-Own/word2vecSGLemma.wordvectors', mmap='r')\n",
    "\n",
    "# Generate document vectors for each document in your corpus\n",
    "size = wv.vector_size\n",
    "document_vectors = np.array([document_vectorizer(doc_token, wv, vector_size=size) for doc_token in corpus])\n",
    "\n",
    "print('Dimensión de nuestra matriz de documentos (ya vectorizados):', document_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = []\n",
    "\n",
    "for doc in data['lemma_description']:\n",
    "    doc_join = ' '.join(doc)\n",
    "    docs.append(doc_join)\n",
    "\n",
    "# Crear un objeto TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "model_google = KeyedVectors.load_word2vec_format('../Data/Word2Vec-Google/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "vectors = model_google.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors_google = np.array([document_vectorizer(doc_token, vectors) for doc_token in corpus])\n",
    "print('Dimensión de nuestra matriz de documentos (ya vectorizados con Google):', document_vectors_google.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Loading..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
